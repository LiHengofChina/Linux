

//======================================================== 
//======================================================== 


"虚拟网卡（Virtual NIC）" 是 "操作系统软件" 层面模拟出来的 "网卡接口"，没有 "真实硬件"。


//======================================================== 常见虚拟网卡类型：
//======================================================== 常见虚拟网卡类型：


lo					//本地回环网卡（127.0.0.1）
docker0				//Docker创建的虚拟网桥
br0, 				//virbr0	虚拟化环境（KVM、Libvirt）里的网桥
tun0, tap0			//VPN、虚拟隧道接口
vethXXX				//容器、网络命名空间内的虚拟对接口


//======================================================== 虚拟网卡的本质特点：
//======================================================== 虚拟网卡的本质特点：


不依赖真实物理硬件
可以有IP地址
可以参与数据转发
通过内核或软件驱动模拟实现


"虚拟网卡就是系统自己用软件模拟出来的‘假网卡’，可以收发数据，但底层还是靠真实物理网卡发出去。"


//======================================================== 物理网卡分 "硬件部分" 和 "软件部分"。
//======================================================== 物理网卡分 "硬件部分" 和 "软件部分"。


（1）硬件部分	//网卡芯片、电路板、PHY模块等，
				//负责信号收发、MAC层处理

（2）软件部分	//驱动程序、内核接口，负责把硬件功能暴露给操作系统，
				//比如让系统识别出 eth0、分配IP、收发数据

//============== 数据流向：
//============== 数据流向：
应用层 → IP层 → 网卡驱动（软件）→ 网卡硬件 → 网络线缆

// "物理网卡硬件负责干活， 软件驱动负责指挥它怎么干活。"


//虚拟网卡就是 "只有指挥层，没有真实干活的硬件"。


//======================================================== 一个 "物理网卡" 有可以有 "多少虚拟网卡" 呢
//======================================================== 一个 "物理网卡" 有可以有 "多少虚拟网卡" 呢


理论上可以有很 "多个虚拟网卡"，数量主要受 "操作系统和内存资源" 限制。

Docker 容器				每启动一个容器，就会创建一对 veth 虚拟网卡
虚拟化（如KVM）			每启动一个虚拟机，就会为它分配一个虚拟网卡
网络命名空间			个网络namespace可以有独立虚拟网卡

//一般Linux内核可以轻松支持几百、上千个虚拟网卡（前提是服务器资源够）。


//======================================================== 怎么看，我的某个虚拟网卡，使用的哪个物理网卡呢
//======================================================== 怎么看，我的某个虚拟网卡，使用的哪个物理网卡呢


不同类型虚拟网卡判断方法：


bridge 网桥 (如 br0, docker0)	//用 brctl show 或 ip link show 看网桥里接了哪些物理网卡
veth对 (容器、namespace常用)	//用 ip link show 查看 veth 的 peer 端是谁，然后再看 peer 最终发向哪个物理网卡
macvlan / ipvlan				//配置时已经指定了用哪个物理网卡，ip link show 可以看到 parent
tun/tap / VPN隧道				//最终出口看路由表，或者用 ip route 和 iptables 查发往外部流量会走哪个网卡

